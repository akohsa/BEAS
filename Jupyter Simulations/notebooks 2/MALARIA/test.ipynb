{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.kaggle.com/sharp1/malaria-cells-classification-through-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byzantine Protection: MultiKrum\n",
    "# Privacy Protection: Gradient Pruning\n",
    "# Aggregation Algorithm: FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install tensorflow_model_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "import tempfile\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout, GaussianNoise\n",
    "import pandas as pd\n",
    "import sys\n",
    "%matplotlib inline\n",
    "from scipy.spatial.distance import euclidean as euc\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import plotly.express as px\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "import random\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS\n",
    "\n",
    "# Blockchain\n",
    "NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# ML\n",
    "Cluster_Size = 100 # max client dataset size for training\n",
    "Batch_Size = 10\n",
    "NUM_Epoch = 3\n",
    "num_classes = 2\n",
    "\n",
    "# Krum\n",
    "krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# Differential Privacy\n",
    "Gaussian_Noise = False\n",
    "Gaussian_Noise_Std_Dev = 0.10\n",
    "\n",
    "Gradient_Clipping = False\n",
    "Clip_Norm = 0.60\n",
    "\n",
    "Gradient_Pruning = True\n",
    "initial_sparsity = 0.00\n",
    "final_sparsity = 0.50\n",
    "\n",
    "backdoor = True\n",
    "poisonedLocals = [1, 2, 3, 4, 5]\n",
    "if backdoor == True:\n",
    "    backdoorAcc = []\n",
    "    backdoorLoss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(filepath, label):\n",
    "    cells = []\n",
    "    labels = []\n",
    "    file = os.listdir(filepath)\n",
    "    for img in file:\n",
    "        try:\n",
    "            image = cv2.imread(filepath + img)\n",
    "            image_from_array = Image.fromarray(image, 'RGB')\n",
    "            size_image = image_from_array.resize((50, 50))\n",
    "            cells.append(np.array(size_image))\n",
    "            labels.append(label)\n",
    "        except AttributeError as e:\n",
    "            print('Skipping file: ', img, e)\n",
    "    print(len(cells), ' Data Points Read!')\n",
    "    return np.array(cells), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reading Training Data')\n",
    "ParasitizedCells, ParasitizedLabels = readData('./input/cell_images/Parasitized/', 1)\n",
    "UninfectedCells, UninfectedLabels  = readData('./input/cell_images/Uninfected/', 0)\n",
    "print('Reading Testing Data')\n",
    "TestParasitizedCells, TestParasitizedLabels = readData('./input/fed/test/Parasitized/', 1)\n",
    "TestUninfectedCells, TestUninfectedLabels  = readData('./input/fed/test/Uninfected/', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Shuffle our dataset\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = numpy.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "Cells = np.concatenate((ParasitizedCells, UninfectedCells))\n",
    "Labels = np.concatenate((ParasitizedLabels, UninfectedLabels))\n",
    "Cells, Labels = unison_shuffled_copies(Cells, Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if backdoor == True:\n",
    "    print('Reading Backdoor Testing Data')\n",
    "    BTestParasitizedCells, BTestParasitizedLabels = readData('./input/backdoor/Parasitized/', 1)\n",
    "    BTestUninfectedCells, BTestUninfectedLabels  = readData('./input/backdoor/Uninfected/', 0)\n",
    "    \n",
    "    BTestCells =np.concatenate((BTestParasitizedCells, BTestUninfectedCells))\n",
    "    BTestLabels = np.concatenate((BTestParasitizedLabels, BTestUninfectedLabels))\n",
    "    \n",
    "    BTestCells, BTestLabels = unison_shuffled_copies(BTestCells, BTestLabels)\n",
    "    \n",
    "    len_BTestData=len(BTestCells)\n",
    "    \n",
    "    (BTestCells)= BTestCells[:(int)(0.1*len_BTestData)]\n",
    "    (BTestLabels)=BTestLabels[:(int)(0.1*len_BTestData)]\n",
    "    \n",
    "    # As we are working on image data we are normalizing data by divinding 255.\n",
    "    BTestCells = BTestCells.astype('float32')/255\n",
    "    #Doing One hot encoding as classifier has multiple classes\n",
    "    BTestLabels=keras.utils.to_categorical(BTestLabels,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(name, Cells, Labels, globalId):\n",
    "    \n",
    "    s = np.arange(Cells.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    Cells = Cells[s]\n",
    "    Labels = Labels[s]\n",
    "    \n",
    "    num_classes=len(np.unique(Labels))\n",
    "    len_data=len(Cells)\n",
    "    print(len_data, ' Data Points')\n",
    "    \n",
    "    (x_train)=Cells\n",
    "    (y_train)=Labels\n",
    "    \n",
    "    # Since we're working on image data, we normalize data by divinding 255.\n",
    "    x_train = x_train.astype('float32')/255 \n",
    "    train_len=len(x_train)\n",
    "    \n",
    "    if poisoned:\n",
    "        x_train[:50] = addBackdoorPattern(x_train[:50])\n",
    "        y_train[:50] = [1 for i in range(50)]\n",
    "        y_train[50:] = [0 for i in range(50)]\n",
    "        \n",
    "    #Doing One hot encoding as classifier has multiple classes\n",
    "    y_train=keras.utils.to_categorical(y_train,num_classes)\n",
    "    \n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    \n",
    "    if Gaussian_Noise == True:\n",
    "        model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "    # model.summary()\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        end_step = np.ceil(len_data / Batch_Size).astype(np.int32) * NUM_Epoch\n",
    "\n",
    "        # Define model for pruning.\n",
    "        pruning_params = {\n",
    "              'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=initial_sparsity,\n",
    "                                                                       final_sparsity=final_sparsity,\n",
    "                                                                       begin_step=0,\n",
    "                                                                       end_step=end_step)\n",
    "        }\n",
    "\n",
    "        logdir = tempfile.mkdtemp()\n",
    "\n",
    "        callbacks = [\n",
    "          tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "          tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "        ]\n",
    "\n",
    "        model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    model.built = True\n",
    "\n",
    "    if globalId != 1:\n",
    "        model.load_weights(\"./weights/global\"+str(globalId)+\".h5\")\n",
    "\n",
    "    if Gradient_Clipping == True:\n",
    "        opt = keras.optimizers.Adam(clipnorm=Clip_Norm)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        history = model.fit(X_train, y_train, batch_size=Batch_Size, epochs=NUM_Epoch, verbose=1, callbacks=callbacks)\n",
    "    else:\n",
    "        history = model.fit(X_train, y_train, batch_size=Batch_Size, epochs=NUM_Epoch, verbose=1)\n",
    "\n",
    "    #Saving Model\n",
    "    model.save(\"./weights/\"+str(name)+\".h5\")\n",
    "    return n_timesteps, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataLen(trainingDict):\n",
    "    n = 0\n",
    "    for w in trainingDict:\n",
    "#         print(w)\n",
    "        n += trainingDict[w]\n",
    "    print('Total number of data points after this round: ', n)\n",
    "    return n\n",
    "\n",
    "def assignWeights(trainingDf, trainingDict):\n",
    "    n = getDataLen(trainingDict)\n",
    "    trainingDf['Weightage'] = trainingDf['DataSize'].apply(lambda x: x/n)\n",
    "    return trainingDf, n\n",
    "    \n",
    "def scale(weight, scaler):\n",
    "    scaledWeights = []\n",
    "    for i in range(len(weight)):\n",
    "        scaledWeights.append(scaler * weight[i])\n",
    "    return scaledWeights\n",
    "\n",
    "def getWeight(d):\n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    if Gaussian_Noise == True:\n",
    "        model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "#     model.summary()\n",
    "    \n",
    "    if Gradient_Pruning == True:\n",
    "        model = prune_low_magnitude(model)\n",
    "    \n",
    "    fpath = \"./weights/\"+d+\".h5\"\n",
    "    model.load_weights(fpath)\n",
    "    weight = model.get_weights()\n",
    "    return weight\n",
    "\n",
    "def getScaledWeight(d, scaler):\n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    if Gaussian_Noise == True:\n",
    "        model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "#     model.summary()\n",
    "    \n",
    "    if Gradient_Pruning == True:\n",
    "        model = prune_low_magnitude(model)\n",
    "    \n",
    "    \n",
    "    fpath = \"./weights/\"+d+\".h5\"\n",
    "    model.load_weights(fpath)\n",
    "    weight = model.get_weights()\n",
    "    return scale(weight, scaler)\n",
    "\n",
    "def avgWeights(scaledWeights):\n",
    "    avg = list()\n",
    "    for weight_list_tuple in zip(*scaledWeights):\n",
    "        layer_mean = tf.math.reduce_sum(weight_list_tuple, axis=0)\n",
    "        avg.append(layer_mean)\n",
    "    return avg\n",
    "\n",
    "def FedAvg(trainingDict):\n",
    "    trainingDf = pd.DataFrame.from_dict(trainingDict, orient='index', columns=['DataSize']) \n",
    "    models = list(trainingDict.keys())\n",
    "    scaledWeights = []\n",
    "    trainingDf, dataLen = assignWeights(trainingDf, trainingDict)\n",
    "    for m in models:\n",
    "        scaledWeights.append(getScaledWeight(m, trainingDf.loc[m]['Weightage']))\n",
    "    fedAvgWeight = avgWeights(scaledWeights)\n",
    "    return fedAvgWeight, dataLen\n",
    "\n",
    "\n",
    "def saveModel(weight, n):\n",
    "    \n",
    "    TestCells = np.concatenate((TestParasitizedCells, TestUninfectedCells))\n",
    "    TestLabels = np.concatenate((TestParasitizedLabels, TestUninfectedLabels))\n",
    "    \n",
    "    sTest = np.arange(TestCells.shape[0])\n",
    "    np.random.shuffle(sTest)\n",
    "    TestCells = TestCells[sTest]\n",
    "    TestLabels = TestLabels[sTest]\n",
    "    \n",
    "    num_classes=len(np.unique(TestLabels))\n",
    "    \n",
    "    (x_test) = TestCells\n",
    "    (y_test) = TestLabels\n",
    "    \n",
    "    # Since we're working on image data, we normalize data by divinding 255.\n",
    "    x_test = x_test.astype('float32')/255\n",
    "    test_len=len(x_test)\n",
    "    \n",
    "    #Doing One hot encoding as classifier has multiple classes\n",
    "    y_test=keras.utils.to_categorical(y_test,num_classes)\n",
    "\n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "#     model.summary()\n",
    "    \n",
    "    if Gradient_Pruning == True:\n",
    "        model = prune_low_magnitude(model)\n",
    "    \n",
    "    \n",
    "    model.set_weights(weight)\n",
    "\n",
    "    # compile the model with loss as categorical_crossentropy and using adam optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    print(\"Loss: \", scores[0])        #Loss\n",
    "    print(\"Accuracy: \", scores[1])    #Accuracy\n",
    "    \n",
    "    if backdoor == True:\n",
    "        Bscores = model.evaluate(BTestCells, BTestLabels)\n",
    "        print(\"Backdoor Loss: \", Bscores[0])        #Loss\n",
    "        print(\"Backdoor Accuracy: \", Bscores[1])    #Accuracy\n",
    "        backdoorLoss.append(Bscores[0])\n",
    "        backdoorAcc.append(Bscores[1])\n",
    "\n",
    "    #Saving Model\n",
    "    fpath = \"./weights/global\"+str(n)+\".h5\"\n",
    "    model.save(fpath)\n",
    "    return scores[0], scores[1]\n",
    "\n",
    "def euclidean(m, n):\n",
    "    distance = []\n",
    "    for i in range(len(m)):\n",
    "#         print(i)\n",
    "        distance.append(euc(m[i].reshape(-1,1), n[i].reshape(-1,1)))\n",
    "#     print(distance)\n",
    "    distance = sum(distance)/len(m)\n",
    "    return distance\n",
    "\n",
    "def MULTIKRUM_merge(trainingDict, b):\n",
    "#     print(trainingDict)\n",
    "    models = list(trainingDict.keys())\n",
    "#     print(models)\n",
    "    trainingDf = pd.DataFrame.from_dict(trainingDict, orient='index', columns=['DataSize'])\n",
    "    l_weights = []\n",
    "    g_weight = {}\n",
    "#     print(models)\n",
    "    for m in models:\n",
    "        print(m)\n",
    "        if 'global' in m:\n",
    "            g_weight['name'] = m\n",
    "            g_weight['weight'] = getWeight(m)\n",
    "        else:\n",
    "            l_weights.append({\n",
    "                'name': m,\n",
    "                'weight': getWeight(m)\n",
    "            })\n",
    "#     print(g_weight)\n",
    "    scores = {}\n",
    "    for m in l_weights:\n",
    "        scores[m['name']] = euclidean(m['weight'], g_weight['weight'])\n",
    "    sortedScores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1])}\n",
    "#     print(scores)\n",
    "#     print(sortedScores)\n",
    "    b = int(len(scores)*b)\n",
    "    \n",
    "    selected = []\n",
    "    \n",
    "    for i in range(b):\n",
    "        selected.append((sortedScores.popitem())[0])\n",
    "\n",
    "    newDict = {}\n",
    "    for i in trainingDict.keys():\n",
    "        if (((i not in selected) and ('global' not in i)) or int(i.replace('local', '') in poisonedLocals)):\n",
    "            newDict[i] = trainingDict[i]\n",
    "\n",
    "    print('Selections: ', newDict)\n",
    "    NewGlobal, dataLen = FedAvg(newDict)\n",
    "    return NewGlobal, dataLen\n",
    "\n",
    "def addBackdoorPattern(imgs):\n",
    "    for img in imgs:\n",
    "        randInt = random.randint(0, 49)\n",
    "        img[randInt][0][0] = 1.\n",
    "        img[randInt][0][1] = 0.\n",
    "        img[randInt][0][2] = 1.\n",
    "        img[randInt][1][0] = -10.\n",
    "        img[randInt][1][1] = 1.\n",
    "        img[randInt][1][2] = -10.\n",
    "        img[randInt][2][0] = -10.\n",
    "        img[randInt][2][1] = -10.\n",
    "        img[randInt][2][2] = 0.\n",
    "        img[randInt][2][0] = 1.\n",
    "        img[randInt][2][1] = 0.\n",
    "        img[randInt][2][2] = 1.\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_local = 0\n",
    "curr_global = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local = {}\n",
    "loss_array = []\n",
    "acc_array = []\n",
    "for i in range(0, len(Cells), Cluster_Size):\n",
    "    if int(curr_global) == 0:\n",
    "        curr_global += 1\n",
    "        name = 'global' + str(curr_global)\n",
    "        l, m = train(name, Cells[i:i+Cluster_Size], Labels[i:i+Cluster_Size], curr_global)\n",
    "        local[name] = l\n",
    "    elif (curr_local != 0) and (int(curr_local)%NUM_Clients == 0):\n",
    "        curr_global += 1\n",
    "        print('Current Global: ', curr_global)\n",
    "        name = 'global' + str(curr_global)\n",
    "        m, l = MULTIKRUM_merge(local, krum_f)\n",
    "        loss, acc = saveModel(m, curr_global)\n",
    "        loss_array.append(loss)\n",
    "        acc_array.append(acc)\n",
    "        curr_local += 1\n",
    "        local = {}\n",
    "        local[name] = l\n",
    "    else:\n",
    "        p = False\n",
    "        print('Current Local: ', curr_local)\n",
    "        if backdoor:\n",
    "            if curr_local in poisonedLocals:\n",
    "                p = True\n",
    "                print('Training Poisoned Local')\n",
    "        name = str('local'+str(curr_local))\n",
    "        curr_local += 1\n",
    "        l, m = train(name, Cells[i:i+Cluster_Size], Labels[i:i+Cluster_Size], curr_global, poisoned = p)\n",
    "        local[name] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS:\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "#\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = False\n",
    "# Gaussian_Noise_Std_Dev = 0.10\n",
    "\n",
    "# Gradient_Clipping = False\n",
    "# Clip_Norm = 0.25\n",
    "\n",
    "# Gradient_Pruning = False\n",
    "# initial_sparsity = 0.50\n",
    "# final_sparsity = 0.80\n",
    "\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS:\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "#\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = False\n",
    "# Gaussian_Noise_Std_Dev = 0.10\n",
    "\n",
    "# Gradient_Clipping = False\n",
    "# Clip_Norm = 0.25\n",
    "\n",
    "# Gradient_Pruning = True <<---\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.50\n",
    "\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 500 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = False\n",
    "# Gaussian_Noise_Std_Dev = 0.10\n",
    "\n",
    "# Gradient_Clipping = False\n",
    "# Clip_Norm = 0.25\n",
    "\n",
    "# Gradient_Pruning = True <-----\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.50\n",
    "\n",
    "# backdoor = True <-----\n",
    "# poisonedLocals = [1, 5, 10, 20, 40, 60, 80, 100] <-----\n",
    "\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = False\n",
    "# Gaussian_Noise_Std_Dev = 0.10\n",
    "\n",
    "# Gradient_Clipping = False\n",
    "# Clip_Norm = 0.25\n",
    "\n",
    "# Gradient_Pruning = False\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.50\n",
    "\n",
    "# backdoor = True <-----\n",
    "# poisonedLocals = [1, 11, 22, 99]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(backdoorAcc)\n",
    "fig = px.line(y=backdoorAcc)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = True <-----\n",
    "# Gaussian_Noise_Std_Dev = 0.10\n",
    "\n",
    "# Gradient_Clipping = False\n",
    "# Clip_Norm = 0.25\n",
    "\n",
    "# Gradient_Pruning = False\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.50\n",
    "\n",
    "# backdoor = True <-----\n",
    "# poisonedLocals = [1, 11, 22, 99]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(backdoorAcc)\n",
    "fig = px.line(y=backdoorAcc)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = False\n",
    "# Gaussian_Noise_Std_Dev = 0.10\n",
    "\n",
    "# Gradient_Clipping = False\n",
    "# Clip_Norm = 0.25\n",
    "\n",
    "# Gradient_Pruning = True <-----\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.50\n",
    "\n",
    "# backdoor = True <-----\n",
    "# poisonedLocals = [1, 11, 22, 99]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(backdoorAcc)\n",
    "fig = px.line(y=backdoorAcc)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = False\n",
    "# Gaussian_Noise_Std_Dev = 0.10\n",
    "\n",
    "# Gradient_Clipping = False\n",
    "# Clip_Norm = 0.25\n",
    "\n",
    "# Gradient_Pruning = True <-----\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.50\n",
    "\n",
    "# backdoor = True <-----\n",
    "# poisonedLocals = [1, 11, 22, 99]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(backdoorAcc)\n",
    "fig = px.line(y=backdoorAcc)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = True <----\n",
    "# Gaussian_Noise_Std_Dev = 0.20\n",
    "\n",
    "# Gradient_Clipping = False\n",
    "# Clip_Norm = 0.25\n",
    "\n",
    "# Gradient_Pruning = False\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.50\n",
    "\n",
    "# backdoor = False\n",
    "# poisonedLocals = [1, 11, 22, 99]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = False \n",
    "# Gaussian_Noise_Std_Dev = 0.20\n",
    "\n",
    "# Gradient_Clipping = True <----\n",
    "# Clip_Norm = 0.75\n",
    "\n",
    "# Gradient_Pruning = False\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.50\n",
    "\n",
    "# backdoor = False\n",
    "# poisonedLocals = [1, 11, 22, 99]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = True <----\n",
    "# Gaussian_Noise_Std_Dev = 0.10\n",
    "\n",
    "# Gradient_Clipping = False\n",
    "# Clip_Norm = 0.25\n",
    "\n",
    "# Gradient_Pruning = False\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.50\n",
    "\n",
    "# backdoor = False\n",
    "# poisonedLocals = [1, 11, 22, 99]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = True <----\n",
    "# Gaussian_Noise_Std_Dev = 0.05\n",
    "\n",
    "# Gradient_Clipping = False\n",
    "# Clip_Norm = 0.25\n",
    "\n",
    "# Gradient_Pruning = False\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.50\n",
    "\n",
    "# backdoor = False\n",
    "# poisonedLocals = [1, 11, 22, 99]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = False\n",
    "# Gaussian_Noise_Std_Dev = 0.05\n",
    "\n",
    "# Gradient_Clipping = True <----\n",
    "# Clip_Norm = 0.90\n",
    "\n",
    "# Gradient_Pruning = False\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.50\n",
    "\n",
    "# backdoor = False\n",
    "# poisonedLocals = [1, 11, 22, 99]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = False\n",
    "# Gaussian_Noise_Std_Dev = 0.05\n",
    "\n",
    "# Gradient_Clipping = True <----\n",
    "# Clip_Norm = 0.60\n",
    "\n",
    "# Gradient_Pruning = False\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.50\n",
    "\n",
    "# backdoor = False\n",
    "# poisonedLocals = [1, 11, 22, 99]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy \n",
    "# Gaussian_Noise = False\n",
    "# Gaussian_Noise_Std_Dev = 0.05\n",
    "\n",
    "# Gradient_Clipping = False \n",
    "# Clip_Norm = 0.60\n",
    "\n",
    "# Gradient_Pruning = True <----\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.25\n",
    "\n",
    "# backdoor = False\n",
    "# poisonedLocals = [1, 11, 22, 99]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy \n",
    "# Gaussian_Noise = False\n",
    "# Gaussian_Noise_Std_Dev = 0.05\n",
    "\n",
    "# Gradient_Clipping = False \n",
    "# Clip_Norm = 0.60\n",
    "\n",
    "# Gradient_Pruning = True <----\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.75\n",
    "\n",
    "# backdoor = False\n",
    "# poisonedLocals = [1, 11, 22, 99]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = False\n",
    "# Gaussian_Noise_Std_Dev = 0.05\n",
    "\n",
    "# Gradient_Clipping = False\n",
    "# Clip_Norm = 0.60\n",
    "\n",
    "# Gradient_Pruning = False\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.75\n",
    "\n",
    "# backdoor = True\n",
    "# poisonedLocals = [1, 2, 3]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(backdoorAcc)\n",
    "fig = px.line(y=backdoorAcc, ya)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = False\n",
    "# Gaussian_Noise_Std_Dev = 0.05\n",
    "\n",
    "# Gradient_Clipping = False\n",
    "# Clip_Norm = 0.60\n",
    "\n",
    "# Gradient_Pruning = False\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.75\n",
    "\n",
    "# backdoor = True\n",
    "# poisonedLocals = [1, 2, 3, 4, 5]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(backdoorAcc)\n",
    "fig = px.line(y=backdoorAcc)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = False\n",
    "# Gaussian_Noise_Std_Dev = 0.05\n",
    "\n",
    "# Gradient_Clipping = False\n",
    "# Clip_Norm = 0.60\n",
    "\n",
    "# Gradient_Pruning = False\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.75\n",
    "\n",
    "# backdoor = True\n",
    "# poisonedLocals = [1, 2, 3, 4, 5]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(backdoorAcc)\n",
    "fig = px.line(y=backdoorAcc)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = True\n",
    "# Gaussian_Noise_Std_Dev = 0.10\n",
    "\n",
    "# Gradient_Clipping = False\n",
    "# Clip_Norm = 0.60\n",
    "\n",
    "# Gradient_Pruning = False\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.75\n",
    "\n",
    "# backdoor = True\n",
    "# poisonedLocals = [1, 2, 3, 4, 5]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(backdoorAcc)\n",
    "fig = px.line(y=backdoorAcc)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = False\n",
    "# Gaussian_Noise_Std_Dev = 0.10\n",
    "\n",
    "# Gradient_Clipping = True\n",
    "# Clip_Norm = 0.60\n",
    "\n",
    "# Gradient_Pruning = False\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.75\n",
    "\n",
    "# backdoor = True\n",
    "# poisonedLocals = [1, 2, 3, 4, 5]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(backdoorAcc)\n",
    "fig = px.line(y=backdoorAcc)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS\n",
    "\n",
    "# # Blockchain\n",
    "# NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# # ML\n",
    "# Cluster_Size = 100 # max client dataset size for training\n",
    "# Batch_Size = 10\n",
    "# NUM_Epoch = 3\n",
    "# num_classes = 2\n",
    "\n",
    "# # Krum\n",
    "# krum_f = 0.25 # percentage of byzantine nodes\n",
    "\n",
    "# # Differential Privacy\n",
    "# Gaussian_Noise = False\n",
    "# Gaussian_Noise_Std_Dev = 0.10\n",
    "\n",
    "# Gradient_Clipping = False\n",
    "# Clip_Norm = 0.60\n",
    "\n",
    "# Gradient_Pruning = True\n",
    "# initial_sparsity = 0.00\n",
    "# final_sparsity = 0.50\n",
    "\n",
    "# backdoor = True\n",
    "# poisonedLocals = [1, 2, 3, 4, 5]\n",
    "print(acc_array)\n",
    "fig = px.line(y=acc_array)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(backdoorAcc)\n",
    "fig = px.line(y=backdoorAcc)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}